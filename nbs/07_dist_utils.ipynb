{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dce6addf-19f5-4217-9c24-fb715a0dccb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp dist_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97f578b9-bee9-449f-aab5-4aa9946275de",
   "metadata": {},
   "source": [
    "# Dist utils\n",
    "> Detail API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b8cc25-01ee-414d-9531-fd3af442ccbf",
   "metadata": {},
   "source": [
    "# Deep NeuralNet pytoch utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a58078e-7ce7-417a-bd09-63b5a79b4e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch\n",
    "from torch import distributed as dist\n",
    "import os\n",
    "def setup_distributed():\n",
    "    local_rank = int(os.environ['LOCAL_RANK']) if 'LOCAL_RANK' in os.environ else 0\n",
    "    n_gpu = int(os.environ['WORLD_SIZE']) if 'WORLD_SIZE' in os.environ else 1\n",
    "    is_distributed = n_gpu > 1\n",
    "    if is_distributed:\n",
    "        torch.cuda.set_device(local_rank)\n",
    "        dist.init_process_group(backend=\"nccl\", init_method=\"env://\")\n",
    "        synchronize()\n",
    "    return is_distributed\n",
    "\n",
    "\n",
    "def get_rank():\n",
    "    if not dist.is_available():\n",
    "        return 0\n",
    "\n",
    "    if not dist.is_initialized():\n",
    "        return 0\n",
    "\n",
    "    return dist.get_rank()\n",
    "\n",
    "\n",
    "def primary():\n",
    "    if not dist.is_available():\n",
    "        return True\n",
    "\n",
    "    if not dist.is_initialized():\n",
    "        return True\n",
    "\n",
    "    return get_rank() == 0\n",
    "\n",
    "\n",
    "def synchronize():\n",
    "    if not dist.is_available():\n",
    "        return\n",
    "\n",
    "    if not dist.is_initialized():\n",
    "        return\n",
    "\n",
    "    world_size = dist.get_world_size()\n",
    "\n",
    "    if world_size == 1:\n",
    "        return\n",
    "\n",
    "    dist.barrier()\n",
    "\n",
    "\n",
    "def get_world_size():\n",
    "    if not dist.is_available():\n",
    "        return 1\n",
    "\n",
    "    if not dist.is_initialized():\n",
    "        return 1\n",
    "\n",
    "    return dist.get_world_size()\n",
    "\n",
    "\n",
    "def reduce_sum(tensor):\n",
    "    if not dist.is_available():\n",
    "        return tensor\n",
    "\n",
    "    if not dist.is_initialized():\n",
    "        return tensor\n",
    "\n",
    "    tensor = tensor.clone()\n",
    "    dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n",
    "\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def gather_grad(params):\n",
    "    world_size = get_world_size()\n",
    "    \n",
    "    if world_size == 1:\n",
    "        return\n",
    "\n",
    "    for param in params:\n",
    "        if param.grad is not None:\n",
    "            dist.all_reduce(param.grad.data, op=dist.ReduceOp.SUM)\n",
    "            param.grad.data.div_(world_size)\n",
    "\n",
    "\n",
    "def all_gather(input, cat=True):\n",
    "    if get_world_size() == 1:\n",
    "        if cat:\n",
    "            return input\n",
    "        else:\n",
    "            return input.unsqueeze(0)\n",
    "    input_list = [torch.zeros_like(input) for _ in range(get_world_size())]\n",
    "    synchronize()\n",
    "    torch.distributed.all_gather(input_list, input, async_op=False)\n",
    "    if cat:\n",
    "        inputs = torch.cat(input_list, dim=0)\n",
    "    else:\n",
    "        inputs = torch.stack(input_list, dim=0)\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def all_gatherv(input, return_boundaries=False):\n",
    "    \"\"\"Variable-sized all_gather\"\"\"\n",
    "\n",
    "    # Broadcast the number of elements in every process:\n",
    "    num_elements = torch.tensor(input.size(0), device=input.device)\n",
    "    num_elements_per_process = all_gather(num_elements, cat=False)\n",
    "    max_elements = num_elements_per_process.max()\n",
    "    # Add padding so every input is the same size:\n",
    "    difference = max_elements - input.size(0)\n",
    "    if difference > 0:\n",
    "        input = torch.cat([input, torch.zeros(difference, *input.size()[1:], device=input.device, dtype=input.dtype)], 0)\n",
    "    inputs = all_gather(input, cat=False)\n",
    "    # Remove padding:\n",
    "    inputs = torch.cat([row[:num_ele] for row, num_ele in zip(inputs, num_elements_per_process)], 0)\n",
    "    if return_boundaries:\n",
    "        boundaries = torch.cumsum(num_elements_per_process, dim=0)\n",
    "        boundaries = torch.cat([torch.zeros(1, device=input.device, dtype=torch.int), boundaries], 0)\n",
    "        return inputs, boundaries.long()\n",
    "    else:\n",
    "        return inputs\n",
    "\n",
    "\n",
    "def all_reduce(input, device):\n",
    "    num_local = torch.tensor([input.size(0)], dtype=torch.float, device=device)\n",
    "    input = input.sum(dim=0, keepdim=True).to(device)\n",
    "    num_global = all_gather(num_local).sum()\n",
    "    input = all_gather(input)\n",
    "    input = input.sum(dim=0).div(num_global)\n",
    "    return input\n",
    "\n",
    "\n",
    "def rank0_to_all(input):\n",
    "    input = all_gather(input)\n",
    "    rank0_input = input[0]\n",
    "    return rank0_input\n",
    "\n",
    "\n",
    "def reduce_loss_dict(loss_dict):\n",
    "    world_size = get_world_size()\n",
    "\n",
    "    if world_size < 2:\n",
    "        return loss_dict\n",
    "\n",
    "    with torch.no_grad():\n",
    "        keys = []\n",
    "        losses = []\n",
    "\n",
    "        for k in sorted(loss_dict.keys()):\n",
    "            keys.append(k)\n",
    "            losses.append(loss_dict[k])\n",
    "\n",
    "        losses = torch.stack(losses, 0)\n",
    "        dist.reduce(losses, dst=0)\n",
    "\n",
    "        if dist.get_rank() == 0:\n",
    "            losses /= world_size\n",
    "\n",
    "        reduced_losses = {k: v for k, v in zip(keys, losses)}\n",
    "\n",
    "    return reduced_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da271a2e-5aa7-4274-8914-c32be99251dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 00_visualize.ipynb.\n",
      "Converted 01_process.ipynb.\n",
      "Converted 03_1_memoize.ipynb.\n",
      "Converted 03_utils.ipynb.\n",
      "Converted 04_debug.ipynb.\n",
      "Converted 05_coco_dataset.ipynb.\n",
      "Converted 06_cli.ipynb.\n",
      "Converted 07_dist_utils.ipynb.\n",
      "Converted all.ipynb.\n",
      "Converted index.ipynb.\n"
     ]
    }
   ],
   "source": [
    "!nbdev_build_lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1726f845-206e-4ef9-a238-fceedc3725c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from avcv import dist_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7633a359-9302-47c3-b206-608ea57ae6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setup_distributed',\n",
       " 'get_rank',\n",
       " 'primary',\n",
       " 'synchronize',\n",
       " 'get_world_size',\n",
       " 'reduce_sum',\n",
       " 'gather_grad',\n",
       " 'all_gather',\n",
       " 'all_gatherv',\n",
       " 'all_reduce',\n",
       " 'rank0_to_all',\n",
       " 'reduce_loss_dict']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_utils.__all__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f5741a-49b3-431b-9d0a-1de00cef3188",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
