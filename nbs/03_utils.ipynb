{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default_exp utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export\n",
    "import os\n",
    "from glob import glob\n",
    "import cv2\n",
    "import os.path as osp\n",
    "from tqdm import tqdm\n",
    "import mmcv\n",
    "from fastcore.script import call_parse, Param\n",
    "from avcv.process import multi_thread\n",
    "from loguru import logger\n",
    "\n",
    "def get_name(path):\n",
    "    path = osp.basename(path).split('.')[:-1]\n",
    "    return '.'.join(path)\n",
    "\n",
    "\n",
    "def find_contours(thresh):\n",
    "    \"\"\"\n",
    "        Get contour of a binary image\n",
    "            Arguments:\n",
    "                thresh: binary image\n",
    "            Returns:\n",
    "                Contours: a list of contour\n",
    "                Hierarchy:\n",
    "\n",
    "    \"\"\"\n",
    "    try:\n",
    "        contours, hierarchy = cv2.findContours(thresh, cv2.RETR_TREE,\n",
    "                                               cv2.CHAIN_APPROX_SIMPLE)\n",
    "        return contours, hierarchy[0]\n",
    "    except:\n",
    "        return None, None\n",
    "\n",
    "\n",
    "@call_parse\n",
    "def download_file_from_google_drive(id_or_link: Param(\"Link or file id\"), destination: Param(\"Path to the save file\")):\n",
    "    if \"https\" in id_or_link:\n",
    "        x = id_or_link\n",
    "        id = x.split(\"/\")[x.split(\"/\").index(\"d\")+1]\n",
    "    else:\n",
    "        id = id_or_link\n",
    "    logger.info(f\"Download from id: {id}\")\n",
    "    import requests\n",
    "\n",
    "    def get_confirm_token(response):\n",
    "        for key, value in response.cookies.items():\n",
    "            if key.startswith('download_warning'):\n",
    "                return value\n",
    "\n",
    "        return None\n",
    "\n",
    "    def save_response_content(response, destination):\n",
    "        CHUNK_SIZE = 32768\n",
    "\n",
    "        with open(destination, \"wb\") as f:\n",
    "            for chunk in response.iter_content(CHUNK_SIZE):\n",
    "                if chunk:  # filter out keep-alive new chunks\n",
    "                    f.write(chunk)\n",
    "\n",
    "    URL = \"https://docs.google.com/uc?export=download\"\n",
    "\n",
    "    session = requests.Session()\n",
    "\n",
    "    response = session.get(URL, params={'id': id}, stream=True)\n",
    "    token = get_confirm_token(response)\n",
    "\n",
    "    if token:\n",
    "        params = {'id': id, 'confirm': token}\n",
    "        response = session.get(URL, params=params, stream=True)\n",
    "\n",
    "    save_response_content(response, destination)\n",
    "    logger.info(f\"Done -> {destination}\")\n",
    "    return osp.abspath(destination)\n",
    "\n",
    "\n",
    "def mkdir(path):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "\n",
    "\n",
    "def put_text(image, pos, text, color=(255, 255, 255)):\n",
    "    return cv2.putText(image, text, pos, cv2.FONT_HERSHEY_SIMPLEX, 1.0,\n",
    "                       color, 2)\n",
    "\n",
    "\n",
    "def images_to_video(\n",
    "        images,\n",
    "        out_path=None,\n",
    "        fps: int = 30,\n",
    "        no_sort=False,\n",
    "        max_num_frame=10e12,\n",
    "        resize_rate=1,\n",
    "        with_text=False,\n",
    "        text_is_date=False,\n",
    "        verbose=True,\n",
    "):\n",
    "\n",
    "    if out_path is None:\n",
    "        assert isinstance(\n",
    "            images, str), \"No out_path specify, you need to input a string to a directory\"\n",
    "        out_path = images+'.mp4'\n",
    "    if isinstance(images, str) and os.path.isdir(images):\n",
    "        from glob import glob\n",
    "        images = glob(os.path.join(images, \"*.jpg\")) + \\\n",
    "            glob(os.path.join(images, \"*.png\")) + \\\n",
    "            glob(os.path.join(images, \"*.jpeg\"))\n",
    "\n",
    "    def get_num(s):\n",
    "        try:\n",
    "            s = os.path.basename(s)\n",
    "            num = int(''.join([c for c in s if c.isdigit()]))\n",
    "        except:\n",
    "            num = s\n",
    "        return num\n",
    "#     global f\n",
    "\n",
    "    def f(img_or_path):\n",
    "        if isinstance(img_or_path, str):\n",
    "            name = os.path.basename(img_or_path)\n",
    "            img = mmcv.imread(img_or_path)\n",
    "            img = cv2.resize(img, output_size)\n",
    "            assert img is not None, img_or_path\n",
    "            if with_text:\n",
    "                if text_is_date:\n",
    "                    from datetime import datetime\n",
    "                    name = name.split('.')[0].split('_')\n",
    "                    f = float('{}.{}'.format(*name))\n",
    "                    name = str(datetime.fromtimestamp(f))\n",
    "                img = put_text(img, (20, 20), name)\n",
    "        else:\n",
    "            img = img_or_path\n",
    "        return img\n",
    "\n",
    "    if not no_sort and isinstance(images[0], str):\n",
    "        images = list(sorted(images, key=get_num))\n",
    "\n",
    "    max_num_frame = int(max_num_frame)\n",
    "    max_num_frame = min(len(images), max_num_frame)\n",
    "\n",
    "    h, w = mmcv.imread(images[0]).shape[:2]\n",
    "    output_size = (int(w*resize_rate), int(h*resize_rate))\n",
    "    if out_path.endswith('.mp4'):\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "        out = cv2.VideoWriter(out_path, fourcc, fps, output_size)\n",
    "    elif out_path.endswith('.avi'):\n",
    "        out = cv2.VideoWriter(\n",
    "            out_path, cv2.VideoWriter_fourcc(*'DIVX'), fps, output_size)\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    images = images[:max_num_frame]\n",
    "    images = multi_thread(f, images, verbose=verbose)\n",
    "    if verbose:\n",
    "        logger.info(\"fWrite video, output_size: {output_size}\")\n",
    "        pbar = mmcv.ProgressBar(len(images))\n",
    "    logger.info(f\"out_path: {out_path}\")\n",
    "    for img in images:\n",
    "        img = cv2.resize(img, output_size)\n",
    "        out.write(img)\n",
    "        if verbose:\n",
    "            pbar.update()\n",
    "    out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "@call_parse\n",
    "def av_i2v(\n",
    "            images: Param(\"Path to the images folder or list of images\"),\n",
    "            out_path: Param(\"Output output video path\", str)=None,\n",
    "            fps: Param(\"Frame per second\", int) = 30,\n",
    "            no_sort: Param(\"Sort images\", bool) = False,\n",
    "            max_num_frame: Param(\"Max num of frame\", int) = 10e12,\n",
    "            resize_rate: Param(\"Resize rate\", float) = 1,\n",
    "            with_text: Param(\"Add additional index to image when writing vidoe\", bool) = False,\n",
    "            text_is_date: Param(\"Add additional index to image when writing vidoe\", bool) = False,\n",
    "            verbose:Param(\"Print...\", bool)=True,\n",
    "        ):\n",
    "\n",
    "    return images_to_video(images, out_path, fps,\n",
    "                           no_sort, max_num_frame, resize_rate, with_text,\n",
    "                           text_is_date,verbose,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "def video_to_images(input_video, output_dir=None, skip=1):\n",
    "    \"\"\"\n",
    "        Extract video to image:\n",
    "            inputs:\n",
    "                input_video: path to video\n",
    "                output_dir: default is set to video name\n",
    "    \"\"\"\n",
    "\n",
    "    if output_dir is None:\n",
    "        vname = get_name(input_video).split('.')[0]\n",
    "        output_dir = osp.join('.cache/video_to_images/', vname)\n",
    "        logger.info(f'Set output_dir = {output_dir}')\n",
    "    \n",
    "    video = mmcv.video.VideoReader(input_video)\n",
    "    pbar = mmcv.ProgressBar(video._frame_cnt)\n",
    "    for i in range(0, len(video), skip):\n",
    "        try:\n",
    "            img = video[i]\n",
    "            out_img_path = os.path.join(output_dir, f'{i:05d}' + '.jpg')\n",
    "            mmcv.imwrite(img, out_img_path)\n",
    "            pbar.update()\n",
    "        except Exception as e:\n",
    "            logger.warning(f\"Cannot write image index {i}, exception: {e}\")\n",
    "            continue\n",
    "    \n",
    "@call_parse\n",
    "def av_v2i(input_video:Param(\"\", str), output_dir:Param(\"\", str)=None, skip:Param(\"\", int)=1):\n",
    "    return video_to_images(input_video, output_dir, skip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video_to_images('../asset/hcm_5s.mp4')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CLI examples\n",
    "+ Download file given a google link\n",
    "\n",
    "        gdown --help\n",
    "        gdown \"https://drive.google.com/file/d/1xOb92Yx3hoOsMsAiI2mnkcyoQatRQNBf/view?usp=sharing\" test.mp3\n",
    "        \n",
    "This should return a openable mp3 file\n",
    "+ Compose a video given a folder of images        \n",
    "\n",
    "        i2v --help\n",
    "        i2v PATH_TO_DIR out.mp4\n",
    "\n",
    "+ Extract images given a video\n",
    "\n",
    "        v2i --help # helper\n",
    "        v2i test.mp4 test-img/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "from mmcv import Timer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class TimeLoger:\n",
    "    def __init__(self):\n",
    "        self.timer = Timer()\n",
    "        self.time_dict = dict()\n",
    "\n",
    "    def start(self):\n",
    "        self.timer.start()\n",
    "\n",
    "    def update(self, name):\n",
    "        # assert not name in self.time_dict\n",
    "        duration = self.timer.since_last_check()\n",
    "        if name in self.time_dict:\n",
    "            self.time_dict[name].append(duration)\n",
    "        else:\n",
    "            self.time_dict[name] = [duration]\n",
    "\n",
    "    def __str__(self):\n",
    "        total_time = np.sum([np.sum(v) for v in self.time_dict.values()])\n",
    "        s = f\"------------------Time Loger Summary : Total {total_time:0.2f} ---------------------:\\n\"\n",
    "        for k, v in self.time_dict.items():\n",
    "            average = np.mean(v)\n",
    "            times = len(v)\n",
    "            percent = np.sum(v)*100/total_time\n",
    "            s += f'\\t\\t{k}:  \\t\\t{percent:0.2f}% ({average:0.4f}s) | Times: {times} \\n'\n",
    "        return s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memoize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "import xxhash\n",
    "import pickle\n",
    "\n",
    "def identify(x):\n",
    "    '''Return an hex digest of the input'''\n",
    "    return xxhash.xxh64(pickle.dumps(x), seed=0).hexdigest()\n",
    "\n",
    "\n",
    "def memoize(func):\n",
    "    import os\n",
    "    import pickle\n",
    "    from functools import wraps\n",
    "    import xxhash\n",
    "    '''Cache result of function call on disk\n",
    "    Support multiple positional and keyword arguments'''\n",
    "    @wraps(func)\n",
    "    def memoized_func(*args, **kwargs):\n",
    "        cache_dir = '.cache'\n",
    "        try:\n",
    "            import inspect\n",
    "            func_id = identify((inspect.getsource(func), args, kwargs))\n",
    "            cache_path = os.path.join(cache_dir, func.__name__+'_'+func_id)\n",
    "\n",
    "            if (os.path.exists(cache_path) and\n",
    "                    not func.__name__ in os.environ and\n",
    "                    not 'BUST_CACHE' in os.environ):\n",
    "                result = pickle.load(open(cache_path, 'rb'))\n",
    "            else:\n",
    "                result = func(*args, **kwargs)\n",
    "                os.makedirs(cache_dir, exist_ok=True)\n",
    "                pickle.dump(result, open(cache_path, 'wb'))\n",
    "            return result\n",
    "        except (KeyError, AttributeError, TypeError, Exception) as e:\n",
    "            logger.warning(f'Exception: {e}, use default function call')\n",
    "            return func(*args, **kwargs)\n",
    "    return memoized_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export\n",
    "# Copyright (c) 2021, NVIDIA CORPORATION & AFFILIATES.  All rights reserved.\n",
    "#\n",
    "# NVIDIA CORPORATION and its licensors retain all intellectual property\n",
    "# and proprietary rights in and to this software, related documentation\n",
    "# and any modifications thereto.  Any use, reproduction, disclosure or\n",
    "# distribution of this software and related documentation without an express\n",
    "# license agreement from NVIDIA CORPORATION is strictly prohibited.\n",
    "\n",
    "\"\"\"Facilities for pickling Python code alongside other data.\n",
    "\n",
    "The pickled code is automatically imported into a separate Python module\n",
    "during unpickling. This way, any previously exported pickles will remain\n",
    "usable even if the original code is no longer available, or if the current\n",
    "version of the code is not consistent with what was originally pickled.\"\"\"\n",
    "\n",
    "import sys\n",
    "import pickle\n",
    "import io\n",
    "import inspect\n",
    "import copy\n",
    "import uuid\n",
    "import types\n",
    "from typing import Any\n",
    "class EasyDict(dict):\n",
    "    \"\"\"Convenience class that behaves like a dict but allows access with the attribute syntax.\"\"\"\n",
    "\n",
    "    def __getattr__(self, name: str) -> Any:\n",
    "        try:\n",
    "            return self[name]\n",
    "        except KeyError:\n",
    "            raise AttributeError(name)\n",
    "\n",
    "    def __setattr__(self, name: str, value: Any) -> None:\n",
    "        self[name] = value\n",
    "\n",
    "    def __delattr__(self, name: str) -> None:\n",
    "        del self[name]\n",
    "\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "_version            = 6         # internal version number\n",
    "_decorators         = set()     # {decorator_class, ...}\n",
    "_import_hooks       = []        # [hook_function, ...]\n",
    "_module_to_src_dict = dict()    # {module: src, ...}\n",
    "_src_to_module_dict = dict()    # {src: module, ...}\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def persistent_class(orig_class):\n",
    "    r\"\"\"Class decorator that extends a given class to save its source code\n",
    "    when pickled.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        from torch_utils import persistence\n",
    "\n",
    "        @persistence.persistent_class\n",
    "        class MyNetwork(torch.nn.Module):\n",
    "            def __init__(self, num_inputs, num_outputs):\n",
    "                super().__init__()\n",
    "                self.fc = MyLayer(num_inputs, num_outputs)\n",
    "                ...\n",
    "\n",
    "        @persistence.persistent_class\n",
    "        class MyLayer(torch.nn.Module):\n",
    "            ...\n",
    "\n",
    "    When pickled, any instance of `MyNetwork` and `MyLayer` will save its\n",
    "    source code alongside other internal state (e.g., parameters, buffers,\n",
    "    and submodules). This way, any previously exported pickle will remain\n",
    "    usable even if the class definitions have been modified or are no\n",
    "    longer available.\n",
    "\n",
    "    The decorator saves the source code of the entire Python module\n",
    "    containing the decorated class. It does *not* save the source code of\n",
    "    any imported modules. Thus, the imported modules must be available\n",
    "    during unpickling, also including `torch_utils.persistence` itself.\n",
    "\n",
    "    It is ok to call functions defined in the same module from the\n",
    "    decorated class. However, if the decorated class depends on other\n",
    "    classes defined in the same module, they must be decorated as well.\n",
    "    This is illustrated in the above example in the case of `MyLayer`.\n",
    "\n",
    "    It is also possible to employ the decorator just-in-time before\n",
    "    calling the constructor. For example:\n",
    "\n",
    "        cls = MyLayer\n",
    "        if want_to_make_it_persistent:\n",
    "            cls = persistence.persistent_class(cls)\n",
    "        layer = cls(num_inputs, num_outputs)\n",
    "\n",
    "    As an additional feature, the decorator also keeps track of the\n",
    "    arguments that were used to construct each instance of the decorated\n",
    "    class. The arguments can be queried via `obj.init_args` and\n",
    "    `obj.init_kwargs`, and they are automatically pickled alongside other\n",
    "    object state. A typical use case is to first unpickle a previous\n",
    "    instance of a persistent class, and then upgrade it to use the latest\n",
    "    version of the source code:\n",
    "\n",
    "        with open('old_pickle.pkl', 'rb') as f:\n",
    "            old_net = pickle.load(f)\n",
    "        new_net = MyNetwork(*old_obj.init_args, **old_obj.init_kwargs)\n",
    "        misc.copy_params_and_buffers(old_net, new_net, require_all=True)\n",
    "    \"\"\"\n",
    "    assert isinstance(orig_class, type)\n",
    "    if is_persistent(orig_class):\n",
    "        return orig_class\n",
    "\n",
    "    assert orig_class.__module__ in sys.modules\n",
    "    orig_module = sys.modules[orig_class.__module__]\n",
    "    orig_module_src = _module_to_src(orig_module)\n",
    "\n",
    "    class Decorator(orig_class):\n",
    "        _orig_module_src = orig_module_src\n",
    "        _orig_class_name = orig_class.__name__\n",
    "\n",
    "        def __init__(self, *args, **kwargs):\n",
    "            super().__init__(*args, **kwargs)\n",
    "            self._init_args = copy.deepcopy(args)\n",
    "            self._init_kwargs = copy.deepcopy(kwargs)\n",
    "            assert orig_class.__name__ in orig_module.__dict__\n",
    "            _check_pickleable(self.__reduce__())\n",
    "\n",
    "        @property\n",
    "        def init_args(self):\n",
    "            return copy.deepcopy(self._init_args)\n",
    "\n",
    "        @property\n",
    "        def init_kwargs(self):\n",
    "            return dnnlib.EasyDict(copy.deepcopy(self._init_kwargs))\n",
    "\n",
    "        def __reduce__(self):\n",
    "            fields = list(super().__reduce__())\n",
    "            fields += [None] * max(3 - len(fields), 0)\n",
    "            if fields[0] is not _reconstruct_persistent_obj:\n",
    "                meta = dict(type='class', version=_version, module_src=self._orig_module_src, class_name=self._orig_class_name, state=fields[2])\n",
    "                fields[0] = _reconstruct_persistent_obj # reconstruct func\n",
    "                fields[1] = (meta,) # reconstruct args\n",
    "                fields[2] = None # state dict\n",
    "            return tuple(fields)\n",
    "\n",
    "    Decorator.__name__ = orig_class.__name__\n",
    "    _decorators.add(Decorator)\n",
    "    return Decorator\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def is_persistent(obj):\n",
    "    r\"\"\"Test whether the given object or class is persistent, i.e.,\n",
    "    whether it will save its source code when pickled.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if obj in _decorators:\n",
    "            return True\n",
    "    except TypeError:\n",
    "        pass\n",
    "    return type(obj) in _decorators # pylint: disable=unidiomatic-typecheck\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def import_hook(hook):\n",
    "    r\"\"\"Register an import hook that is called whenever a persistent object\n",
    "    is being unpickled. A typical use case is to patch the pickled source\n",
    "    code to avoid errors and inconsistencies when the API of some imported\n",
    "    module has changed.\n",
    "\n",
    "    The hook should have the following signature:\n",
    "\n",
    "        hook(meta) -> modified meta\n",
    "\n",
    "    `meta` is an instance of `dnnlib.EasyDict` with the following fields:\n",
    "\n",
    "        type:       Type of the persistent object, e.g. `'class'`.\n",
    "        version:    Internal version number of `torch_utils.persistence`.\n",
    "        module_src  Original source code of the Python module.\n",
    "        class_name: Class name in the original Python module.\n",
    "        state:      Internal state of the object.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        @persistence.import_hook\n",
    "        def wreck_my_network(meta):\n",
    "            if meta.class_name == 'MyNetwork':\n",
    "                print('MyNetwork is being imported. I will wreck it!')\n",
    "                meta.module_src = meta.module_src.replace(\"True\", \"False\")\n",
    "            return meta\n",
    "    \"\"\"\n",
    "    assert callable(hook)\n",
    "    _import_hooks.append(hook)\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def _reconstruct_persistent_obj(meta):\n",
    "    r\"\"\"Hook that is called internally by the `pickle` module to unpickle\n",
    "    a persistent object.\n",
    "    \"\"\"\n",
    "    meta = EasyDict(meta)\n",
    "    meta.state = EasyDict(meta.state)\n",
    "    for hook in _import_hooks:\n",
    "        meta = hook(meta)\n",
    "        assert meta is not None\n",
    "\n",
    "    assert meta.version == _version\n",
    "    module = _src_to_module(meta.module_src)\n",
    "\n",
    "    assert meta.type == 'class'\n",
    "    orig_class = module.__dict__[meta.class_name]\n",
    "    decorator_class = persistent_class(orig_class)\n",
    "    obj = decorator_class.__new__(decorator_class)\n",
    "\n",
    "    setstate = getattr(obj, '__setstate__', None)\n",
    "    if callable(setstate):\n",
    "        setstate(meta.state) # pylint: disable=not-callable\n",
    "    else:\n",
    "        obj.__dict__.update(meta.state)\n",
    "    return obj\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def _module_to_src(module):\n",
    "    r\"\"\"Query the source code of a given Python module.\n",
    "    \"\"\"\n",
    "    src = _module_to_src_dict.get(module, None)\n",
    "    if src is None:\n",
    "        src = inspect.getsource(module)\n",
    "        _module_to_src_dict[module] = src\n",
    "        _src_to_module_dict[src] = module\n",
    "    return src\n",
    "\n",
    "def _src_to_module(src):\n",
    "    r\"\"\"Get or create a Python module for the given source code.\n",
    "    \"\"\"\n",
    "    module = _src_to_module_dict.get(src, None)\n",
    "    if module is None:\n",
    "        module_name = \"_imported_module_\" + uuid.uuid4().hex\n",
    "        module = types.ModuleType(module_name)\n",
    "        sys.modules[module_name] = module\n",
    "        _module_to_src_dict[module] = src\n",
    "        _src_to_module_dict[src] = module\n",
    "        exec(src, module.__dict__) # pylint: disable=exec-used\n",
    "    return module\n",
    "\n",
    "#----------------------------------------------------------------------------\n",
    "\n",
    "def _check_pickleable(obj):\n",
    "    r\"\"\"Check that the given object is pickleable, raising an exception if\n",
    "    it is not. This function is expected to be considerably more efficient\n",
    "    than actually pickling the object.\n",
    "    \"\"\"\n",
    "    def recurse(obj):\n",
    "        if isinstance(obj, (list, tuple, set)):\n",
    "            return [recurse(x) for x in obj]\n",
    "        if isinstance(obj, dict):\n",
    "            return [[recurse(x), recurse(y)] for x, y in obj.items()]\n",
    "        if isinstance(obj, (str, int, float, bool, bytes, bytearray)):\n",
    "            return None # Python primitive types are pickleable.\n",
    "        if f'{type(obj).__module__}.{type(obj).__name__}' in ['numpy.ndarray', 'torch.Tensor', 'torch.nn.parameter.Parameter']:\n",
    "            return None # NumPy arrays and PyTorch tensors are pickleable.\n",
    "        if is_persistent(obj):\n",
    "            return None # Persistent objects are pickleable, by virtue of the constructor check.\n",
    "        return obj\n",
    "    with io.BytesIO() as f:\n",
    "        pickle.dump(recurse(obj), f)\n",
    "\n",
    "#----------------------------------------------------------------------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install dnnlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
